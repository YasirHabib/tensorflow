{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fashion_mnist_pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YasirHabib/tensorflow/blob/master/fashion_mnist_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWomXYWNlC_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oHGTshDlI9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch import optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITGTAiF2lMMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ANN():\n",
        "  def __init__(self, size):\n",
        "    self.size = size\n",
        "    \n",
        "  def fit(self, Xtrain, Ytrain, Xtest, Ytest, training_epochs=10, batch_sz=500):\n",
        "    \n",
        "    N,D = Xtrain.shape\n",
        "    K = len(set(Ytrain))\n",
        "    \n",
        "    # convert the data arrays into torch tensors\n",
        "    Xtrain = torch.from_numpy(Xtrain).float()\n",
        "    Ytrain = torch.from_numpy(Ytrain).long()\n",
        "    Xtest = torch.from_numpy(Xtest).float()\n",
        "    Ytest = torch.from_numpy(Ytest).long()\n",
        "    \n",
        "    model = torch.nn.Sequential()\n",
        "    \n",
        "    model.add_module(\"dense1\", torch.nn.Linear(D,self.size))\n",
        "    model.add_module(\"relu1\", torch.nn.ReLU())\n",
        "    model.add_module(\"dense2\", torch.nn.Linear(self.size,K))\n",
        "    # Note: no final softmax!\n",
        "    # just like Tensorflow, it's included in cross-entropy function\n",
        "    \n",
        "    loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    \n",
        "    def train(model, loss, optimizer, inputs, labels):\n",
        "      inputs = Variable(inputs, requires_grad=False)\n",
        "      labels = Variable(labels, requires_grad=False)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # get output from the model, given the inputs\n",
        "      logits = model.forward(inputs)\n",
        "      \n",
        "      # get loss for the predicted output\n",
        "      cost = loss.forward(logits, labels)\n",
        "      \n",
        "      # get gradients w.r.t to parameters\n",
        "      cost.backward()\n",
        "      \n",
        "      # update parameters\n",
        "      optimizer.step()\n",
        "      \n",
        "      return cost.item()\n",
        "    \n",
        "    def test(model, loss, inputs, labels):\n",
        "      inputs = Variable(inputs, requires_grad=False)\n",
        "      labels = Variable(labels, requires_grad=False)\n",
        "      \n",
        "      logits = model.forward(inputs)\n",
        "      cost = loss.forward(logits, labels)\n",
        "      \n",
        "      return cost.item()\n",
        "    \n",
        "    def predict(model, inputs):\n",
        "      inputs = Variable(inputs, requires_grad=False)\n",
        "      \n",
        "      logits = model.forward(inputs)\n",
        "      return logits.data.numpy().argmax(axis=1)\n",
        "      \n",
        "    n_batches = Xtrain.size()[0] // batch_sz\n",
        "    train_costs = []\n",
        "    test_costs = []\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "    \n",
        "    for epoch in range(training_epochs):\n",
        "      for j in range(n_batches):\n",
        "        Xbatch = Xtrain[j*batch_sz:(j*batch_sz + batch_sz)]\n",
        "        Ybatch = Ytrain[j*batch_sz:(j*batch_sz + batch_sz)]\n",
        "        \n",
        "        train_c=train(model, loss, optimizer, Xbatch, Ybatch)\n",
        "        train_costs.append(train_c)\n",
        "        \n",
        "        if j%10 == 0:\n",
        "          test_c=test(model, loss, Xtest, Ytest)\n",
        "          test_costs.append(test_c)\n",
        "          \n",
        "          p_train = predict(model, Xtrain)\n",
        "          p_test = predict(model, Xtest)\n",
        "          \n",
        "          acc_train = np.mean(p_train == Ytrain.numpy())\n",
        "          train_accuracies.append(acc_train)\n",
        "          \n",
        "          acc_test = np.mean(p_test == Ytest.numpy())\n",
        "          test_accuracies.append(acc_test)\n",
        "          \n",
        "          print(\"Epoch\", (epoch+1), \"Training Cost\", train_c, \"Test cost\", test_c, \"Training Acc\", acc_train, \"Test Acc\", acc_test)\n",
        "    \n",
        "    plt.plot(train_costs, label='Training Cost')\n",
        "    plt.plot(test_costs, label='Test Cost')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ecftol1kpNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  \n",
        "  df_train = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/fashion-mnist_train.csv')\n",
        "  df_test = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/fashion-mnist_test.csv')\n",
        "  \n",
        "  #print(df_train.dtypes)\n",
        "  #print(df_train.columns)\n",
        "  #print(df_train.info())\n",
        "  #print(df_train.head())\n",
        "  #print(df_train.head)\n",
        "  \n",
        "  data_train = df_train.values\n",
        "  data_test = df_test.values\n",
        "  \n",
        "  Xtrain = data_train[:,1:]\n",
        "  Ytrain = data_train[:,0]\n",
        "  \n",
        "  Xtest = data_test[:,1:]\n",
        "  Ytest = data_test[:,0]\n",
        "  \n",
        "    # Normalization\n",
        "  Xtrain = (Xtrain - Xtrain.mean(axis=0)) / Xtrain.std(axis=0)\n",
        "  Xtest = (Xtest - Xtest.mean(axis=0)) / Xtest.std(axis=0)\n",
        "  \n",
        "  model = ANN(128)\n",
        "  model.fit(Xtrain, Ytrain, Xtest, Ytest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lYsAup3k334",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de21ea65-37f9-4e83-83f2-073c2a32f768"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "120\n",
            "Epoch 1 Training Cost 2.3584961891174316 Test cost 2.000669479370117 Training Acc 0.4635666666666667 Test Acc 0.4602\n",
            "Epoch 1 Training Cost 0.9099036455154419 Test cost 0.8380125761032104 Training Acc 0.707 Test Acc 0.7037\n",
            "Epoch 1 Training Cost 0.6799651980400085 Test cost 0.6521740555763245 Training Acc 0.7696666666666667 Test Acc 0.7705\n",
            "Epoch 1 Training Cost 0.5420529246330261 Test cost 0.5719971656799316 Training Acc 0.7932333333333333 Test Acc 0.7957\n",
            "Epoch 1 Training Cost 0.5249855518341064 Test cost 0.5216034650802612 Training Acc 0.8172 Test Acc 0.8203\n",
            "Epoch 1 Training Cost 0.43691790103912354 Test cost 0.48843178153038025 Training Acc 0.8255666666666667 Test Acc 0.8267\n",
            "Epoch 1 Training Cost 0.45172053575515747 Test cost 0.4679364860057831 Training Acc 0.8339666666666666 Test Acc 0.8363\n",
            "Epoch 1 Training Cost 0.46068075299263 Test cost 0.45302388072013855 Training Acc 0.8394833333333334 Test Acc 0.839\n",
            "Epoch 1 Training Cost 0.44299185276031494 Test cost 0.43774423003196716 Training Acc 0.8451833333333333 Test Acc 0.8463\n",
            "Epoch 1 Training Cost 0.47514745593070984 Test cost 0.4353640079498291 Training Acc 0.8457166666666667 Test Acc 0.8469\n",
            "Epoch 1 Training Cost 0.4513658583164215 Test cost 0.4241638779640198 Training Acc 0.85155 Test Acc 0.8526\n",
            "Epoch 1 Training Cost 0.47085678577423096 Test cost 0.42062851786613464 Training Acc 0.8543 Test Acc 0.857\n",
            "Epoch 2 Training Cost 0.4463587701320648 Test cost 0.411893755197525 Training Acc 0.8575166666666667 Test Acc 0.8544\n",
            "Epoch 2 Training Cost 0.3852387070655823 Test cost 0.4055696129798889 Training Acc 0.8576333333333334 Test Acc 0.8579\n",
            "Epoch 2 Training Cost 0.41973933577537537 Test cost 0.39667457342147827 Training Acc 0.8618666666666667 Test Acc 0.8616\n",
            "Epoch 2 Training Cost 0.34163203835487366 Test cost 0.39640486240386963 Training Acc 0.8628333333333333 Test Acc 0.8627\n",
            "Epoch 2 Training Cost 0.368794709444046 Test cost 0.3913922607898712 Training Acc 0.8637166666666667 Test Acc 0.8619\n",
            "Epoch 2 Training Cost 0.33718395233154297 Test cost 0.38361266255378723 Training Acc 0.8692666666666666 Test Acc 0.8628\n",
            "Epoch 2 Training Cost 0.34882649779319763 Test cost 0.38309189677238464 Training Acc 0.8689333333333333 Test Acc 0.8641\n",
            "Epoch 2 Training Cost 0.380103200674057 Test cost 0.37948575615882874 Training Acc 0.87045 Test Acc 0.8659\n",
            "Epoch 2 Training Cost 0.3546335995197296 Test cost 0.374245822429657 Training Acc 0.8733333333333333 Test Acc 0.8689\n",
            "Epoch 2 Training Cost 0.39281386137008667 Test cost 0.37820765376091003 Training Acc 0.8714 Test Acc 0.8674\n",
            "Epoch 2 Training Cost 0.38934338092803955 Test cost 0.3728564381599426 Training Acc 0.8733333333333333 Test Acc 0.868\n",
            "Epoch 2 Training Cost 0.40799835324287415 Test cost 0.37199294567108154 Training Acc 0.8746 Test Acc 0.8686\n",
            "Epoch 3 Training Cost 0.39204955101013184 Test cost 0.3651966154575348 Training Acc 0.8780666666666667 Test Acc 0.8698\n",
            "Epoch 3 Training Cost 0.3379726707935333 Test cost 0.361807644367218 Training Acc 0.87855 Test Acc 0.8707\n",
            "Epoch 3 Training Cost 0.35267430543899536 Test cost 0.3557722866535187 Training Acc 0.8789666666666667 Test Acc 0.8729\n",
            "Epoch 3 Training Cost 0.29564565420150757 Test cost 0.3616141974925995 Training Acc 0.8787666666666667 Test Acc 0.873\n",
            "Epoch 3 Training Cost 0.3153432309627533 Test cost 0.355780690908432 Training Acc 0.8800166666666667 Test Acc 0.8726\n",
            "Epoch 3 Training Cost 0.2990553677082062 Test cost 0.35167208313941956 Training Acc 0.8828 Test Acc 0.8708\n",
            "Epoch 3 Training Cost 0.30371716618537903 Test cost 0.3527357280254364 Training Acc 0.88225 Test Acc 0.8727\n",
            "Epoch 3 Training Cost 0.3453141450881958 Test cost 0.35258933901786804 Training Acc 0.8832166666666666 Test Acc 0.8727\n",
            "Epoch 3 Training Cost 0.3204897344112396 Test cost 0.34864166378974915 Training Acc 0.8844 Test Acc 0.8759\n",
            "Epoch 3 Training Cost 0.3535281717777252 Test cost 0.3524024784564972 Training Acc 0.8837666666666667 Test Acc 0.8742\n",
            "Epoch 3 Training Cost 0.35503971576690674 Test cost 0.3495837152004242 Training Acc 0.8849833333333333 Test Acc 0.8754\n",
            "Epoch 3 Training Cost 0.37642428278923035 Test cost 0.3497181832790375 Training Acc 0.88525 Test Acc 0.8758\n",
            "Epoch 4 Training Cost 0.3580305874347687 Test cost 0.34294620156288147 Training Acc 0.88985 Test Acc 0.8748\n",
            "Epoch 4 Training Cost 0.30986130237579346 Test cost 0.34108641743659973 Training Acc 0.8897 Test Acc 0.8768\n",
            "Epoch 4 Training Cost 0.3148512840270996 Test cost 0.33542659878730774 Training Acc 0.89 Test Acc 0.8805\n",
            "Epoch 4 Training Cost 0.27027779817581177 Test cost 0.34383171796798706 Training Acc 0.88805 Test Acc 0.8782\n",
            "Epoch 4 Training Cost 0.28745749592781067 Test cost 0.33734816312789917 Training Acc 0.8891 Test Acc 0.8785\n",
            "Epoch 4 Training Cost 0.2759745419025421 Test cost 0.33439257740974426 Training Acc 0.8914166666666666 Test Acc 0.8765\n",
            "Epoch 4 Training Cost 0.2724529504776001 Test cost 0.33536651730537415 Training Acc 0.8911166666666667 Test Acc 0.8786\n",
            "Epoch 4 Training Cost 0.3214901387691498 Test cost 0.3370169997215271 Training Acc 0.8921666666666667 Test Acc 0.876\n",
            "Epoch 4 Training Cost 0.29366281628608704 Test cost 0.33311548829078674 Training Acc 0.8926833333333334 Test Acc 0.8811\n",
            "Epoch 4 Training Cost 0.3261740207672119 Test cost 0.3366815149784088 Training Acc 0.89205 Test Acc 0.8777\n",
            "Epoch 4 Training Cost 0.3300730288028717 Test cost 0.33542510867118835 Training Acc 0.89325 Test Acc 0.8779\n",
            "Epoch 4 Training Cost 0.35132384300231934 Test cost 0.3362739086151123 Training Acc 0.8935833333333333 Test Acc 0.8806\n",
            "Epoch 5 Training Cost 0.3358926773071289 Test cost 0.3300890028476715 Training Acc 0.8971833333333333 Test Acc 0.8801\n",
            "Epoch 5 Training Cost 0.2852334976196289 Test cost 0.3285762071609497 Training Acc 0.8969333333333334 Test Acc 0.8812\n",
            "Epoch 5 Training Cost 0.28936728835105896 Test cost 0.3224724233150482 Training Acc 0.89705 Test Acc 0.8846\n",
            "Epoch 5 Training Cost 0.24976804852485657 Test cost 0.3314608931541443 Training Acc 0.8957166666666667 Test Acc 0.8808\n",
            "Epoch 5 Training Cost 0.26549452543258667 Test cost 0.32616177201271057 Training Acc 0.8959333333333334 Test Acc 0.8821\n",
            "Epoch 5 Training Cost 0.2573074698448181 Test cost 0.3227175772190094 Training Acc 0.89875 Test Acc 0.8814\n",
            "Epoch 5 Training Cost 0.2518779933452606 Test cost 0.32290932536125183 Training Acc 0.8987 Test Acc 0.884\n",
            "Epoch 5 Training Cost 0.3023427724838257 Test cost 0.3266875147819519 Training Acc 0.8988 Test Acc 0.8791\n",
            "Epoch 5 Training Cost 0.2742122709751129 Test cost 0.3222479820251465 Training Acc 0.9006666666666666 Test Acc 0.8842\n",
            "Epoch 5 Training Cost 0.30308523774147034 Test cost 0.3267667591571808 Training Acc 0.8985833333333333 Test Acc 0.8811\n",
            "Epoch 5 Training Cost 0.310686320066452 Test cost 0.32581618428230286 Training Acc 0.8997833333333334 Test Acc 0.8827\n",
            "Epoch 5 Training Cost 0.3304536044597626 Test cost 0.327226847410202 Training Acc 0.9001333333333333 Test Acc 0.8823\n",
            "Epoch 6 Training Cost 0.31864047050476074 Test cost 0.3210776746273041 Training Acc 0.9042 Test Acc 0.8821\n",
            "Epoch 6 Training Cost 0.26482924818992615 Test cost 0.3183525800704956 Training Acc 0.9032333333333333 Test Acc 0.8847\n",
            "Epoch 6 Training Cost 0.2658030390739441 Test cost 0.314520001411438 Training Acc 0.9029333333333334 Test Acc 0.8871\n",
            "Epoch 6 Training Cost 0.23490074276924133 Test cost 0.32282060384750366 Training Acc 0.9020333333333334 Test Acc 0.8839\n",
            "Epoch 6 Training Cost 0.24980591237545013 Test cost 0.3186395764350891 Training Acc 0.9019666666666667 Test Acc 0.8834\n",
            "Epoch 6 Training Cost 0.24126411974430084 Test cost 0.3154190182685852 Training Acc 0.9036 Test Acc 0.8836\n",
            "Epoch 6 Training Cost 0.23526276648044586 Test cost 0.3149961233139038 Training Acc 0.9044333333333333 Test Acc 0.8866\n",
            "Epoch 6 Training Cost 0.2847774922847748 Test cost 0.31918084621429443 Training Acc 0.9045666666666666 Test Acc 0.8811\n",
            "Epoch 6 Training Cost 0.2579711377620697 Test cost 0.3139369487762451 Training Acc 0.9067833333333334 Test Acc 0.8872\n",
            "Epoch 6 Training Cost 0.28158679604530334 Test cost 0.31984296441078186 Training Acc 0.9044166666666666 Test Acc 0.8844\n",
            "Epoch 6 Training Cost 0.2936554551124573 Test cost 0.31961390376091003 Training Acc 0.9052333333333333 Test Acc 0.8854\n",
            "Epoch 6 Training Cost 0.31038540601730347 Test cost 0.3211424648761749 Training Acc 0.9054166666666666 Test Acc 0.883\n",
            "Epoch 7 Training Cost 0.3004539906978607 Test cost 0.3147968649864197 Training Acc 0.9092 Test Acc 0.8838\n",
            "Epoch 7 Training Cost 0.2452959567308426 Test cost 0.3123069405555725 Training Acc 0.9085333333333333 Test Acc 0.888\n",
            "Epoch 7 Training Cost 0.25096559524536133 Test cost 0.30960923433303833 Training Acc 0.9085166666666666 Test Acc 0.888\n",
            "Epoch 7 Training Cost 0.22062906622886658 Test cost 0.316887229681015 Training Acc 0.9081833333333333 Test Acc 0.8852\n",
            "Epoch 7 Training Cost 0.23777514696121216 Test cost 0.3135519027709961 Training Acc 0.9073666666666667 Test Acc 0.8849\n",
            "Epoch 7 Training Cost 0.22788190841674805 Test cost 0.31018033623695374 Training Acc 0.9089166666666667 Test Acc 0.8865\n",
            "Epoch 7 Training Cost 0.22243236005306244 Test cost 0.3091653287410736 Training Acc 0.9101666666666667 Test Acc 0.8878\n",
            "Epoch 7 Training Cost 0.2705281674861908 Test cost 0.3146977722644806 Training Acc 0.9092333333333333 Test Acc 0.8832\n",
            "Epoch 7 Training Cost 0.24400098621845245 Test cost 0.30880168080329895 Training Acc 0.91265 Test Acc 0.8897\n",
            "Epoch 7 Training Cost 0.2642369866371155 Test cost 0.316255658864975 Training Acc 0.9086 Test Acc 0.8851\n",
            "Epoch 7 Training Cost 0.27824151515960693 Test cost 0.3156188726425171 Training Acc 0.9101166666666667 Test Acc 0.8866\n",
            "Epoch 7 Training Cost 0.29123246669769287 Test cost 0.3174351453781128 Training Acc 0.9099 Test Acc 0.8844\n",
            "Epoch 8 Training Cost 0.28672030568122864 Test cost 0.31071072816848755 Training Acc 0.91395 Test Acc 0.8856\n",
            "Epoch 8 Training Cost 0.23119257390499115 Test cost 0.307972252368927 Training Acc 0.9142833333333333 Test Acc 0.889\n",
            "Epoch 8 Training Cost 0.2389547973871231 Test cost 0.30697664618492126 Training Acc 0.9140333333333334 Test Acc 0.8875\n",
            "Epoch 8 Training Cost 0.20919430255889893 Test cost 0.3136537969112396 Training Acc 0.9126333333333333 Test Acc 0.8859\n",
            "Epoch 8 Training Cost 0.22381460666656494 Test cost 0.3097768723964691 Training Acc 0.9125666666666666 Test Acc 0.8874\n",
            "Epoch 8 Training Cost 0.21618834137916565 Test cost 0.3059937059879303 Training Acc 0.91355 Test Acc 0.888\n",
            "Epoch 8 Training Cost 0.21107512712478638 Test cost 0.3058866560459137 Training Acc 0.9150166666666667 Test Acc 0.8895\n",
            "Epoch 8 Training Cost 0.25778332352638245 Test cost 0.31241777539253235 Training Acc 0.9136833333333333 Test Acc 0.8855\n",
            "Epoch 8 Training Cost 0.23452769219875336 Test cost 0.3051851987838745 Training Acc 0.9169333333333334 Test Acc 0.891\n",
            "Epoch 8 Training Cost 0.24721461534500122 Test cost 0.3145965039730072 Training Acc 0.91285 Test Acc 0.8863\n",
            "Epoch 8 Training Cost 0.2657918632030487 Test cost 0.31411507725715637 Training Acc 0.9141 Test Acc 0.8873\n",
            "Epoch 8 Training Cost 0.27526214718818665 Test cost 0.314512699842453 Training Acc 0.91435 Test Acc 0.8848\n",
            "Epoch 9 Training Cost 0.2739295959472656 Test cost 0.30848705768585205 Training Acc 0.9177 Test Acc 0.8853\n",
            "Epoch 9 Training Cost 0.2151816338300705 Test cost 0.3057776689529419 Training Acc 0.9184333333333333 Test Acc 0.8897\n",
            "Epoch 9 Training Cost 0.22749383747577667 Test cost 0.30582839250564575 Training Acc 0.9178333333333333 Test Acc 0.8887\n",
            "Epoch 9 Training Cost 0.19697155058383942 Test cost 0.31201285123825073 Training Acc 0.9163833333333333 Test Acc 0.888\n",
            "Epoch 9 Training Cost 0.21192999184131622 Test cost 0.3072883188724518 Training Acc 0.9166666666666666 Test Acc 0.8877\n",
            "Epoch 9 Training Cost 0.20482823252677917 Test cost 0.3030019998550415 Training Acc 0.9185166666666666 Test Acc 0.8897\n",
            "Epoch 9 Training Cost 0.20015935599803925 Test cost 0.303726464509964 Training Acc 0.91835 Test Acc 0.8908\n",
            "Epoch 9 Training Cost 0.24637983739376068 Test cost 0.311148464679718 Training Acc 0.9181166666666667 Test Acc 0.8863\n",
            "Epoch 9 Training Cost 0.2232871949672699 Test cost 0.3031010627746582 Training Acc 0.9205333333333333 Test Acc 0.8925\n",
            "Epoch 9 Training Cost 0.2303691804409027 Test cost 0.31356221437454224 Training Acc 0.91665 Test Acc 0.8884\n",
            "Epoch 9 Training Cost 0.2542111277580261 Test cost 0.314726859331131 Training Acc 0.91685 Test Acc 0.8879\n",
            "Epoch 9 Training Cost 0.2590218782424927 Test cost 0.31194987893104553 Training Acc 0.9180333333333334 Test Acc 0.8858\n",
            "Epoch 10 Training Cost 0.2614988386631012 Test cost 0.30941158533096313 Training Acc 0.9209333333333334 Test Acc 0.885\n",
            "Epoch 10 Training Cost 0.20273904502391815 Test cost 0.3047288954257965 Training Acc 0.92245 Test Acc 0.8901\n",
            "Epoch 10 Training Cost 0.21522165834903717 Test cost 0.304578572511673 Training Acc 0.9223166666666667 Test Acc 0.8918\n",
            "Epoch 10 Training Cost 0.18902358412742615 Test cost 0.3127000033855438 Training Acc 0.91985 Test Acc 0.8884\n",
            "Epoch 10 Training Cost 0.20190024375915527 Test cost 0.3069974184036255 Training Acc 0.92025 Test Acc 0.8895\n",
            "Epoch 10 Training Cost 0.19470301270484924 Test cost 0.30086180567741394 Training Acc 0.9232333333333334 Test Acc 0.8924\n",
            "Epoch 10 Training Cost 0.189273402094841 Test cost 0.3028310239315033 Training Acc 0.92205 Test Acc 0.892\n",
            "Epoch 10 Training Cost 0.23626406490802765 Test cost 0.3106432557106018 Training Acc 0.9216666666666666 Test Acc 0.8873\n",
            "Epoch 10 Training Cost 0.21902798116207123 Test cost 0.3023332357406616 Training Acc 0.9240666666666667 Test Acc 0.8935\n",
            "Epoch 10 Training Cost 0.21296176314353943 Test cost 0.3101673722267151 Training Acc 0.9219166666666667 Test Acc 0.8904\n",
            "Epoch 10 Training Cost 0.24331220984458923 Test cost 0.31670546531677246 Training Acc 0.9199833333333334 Test Acc 0.888\n",
            "Epoch 10 Training Cost 0.24450060725212097 Test cost 0.3089146018028259 Training Acc 0.9225666666666666 Test Acc 0.8877\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-174f78df48e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-8b6053982973>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mANN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-ba00436ef626>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, Xtrain, Ytrain, Xtest, Ytest, training_epochs, batch_sz)\u001b[0m\n\u001b[1;32m     91\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Training Cost\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Test cost\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Training Acc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Test Acc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_costs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Cost'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_costs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Test Cost'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2809\u001b[0m     return gca().plot(\n\u001b[1;32m   2810\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2811\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m                                   \"with non-matching shapes is deprecated.\")\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mncx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mncy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m             \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_makeline\u001b[0;34m(self, x, y, kw, kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mdefault_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getdefaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setdefaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, xdata, ydata, linewidth, linestyle, color, marker, markersize, markeredgewidth, markeredgecolor, markerfacecolor, markerfacecoloralt, fillstyle, antialiased, dash_capstyle, solid_capstyle, dash_joinstyle, solid_joinstyle, pickradius, drawstyle, markevery, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# update kwargs before updating data to give the caller a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;31m# chance to init axes (and hence unit support)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickradius\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickradius\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mind_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, props)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meventson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_update_property\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meventson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_update_property\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36m_update_property\u001b[0;34m(self, k, v)\u001b[0m\n\u001b[1;32m    910\u001b[0m                 \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'set_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown property %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Unknown property labels"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYfAlqQmDL0lNGHxJauJ/Acz2XLpusNoKAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}